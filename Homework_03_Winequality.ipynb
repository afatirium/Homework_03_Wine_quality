{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a85b633",
   "metadata": {},
   "source": [
    "# Exercises - Multi-Class Classification with Boosting Algorithms - White Wine Quality\n",
    "\n",
    "In this exercise, I will ask you to use the white wine quality datasets (link: https://archive.ics.uci.edu/dataset/186/wine+quality) for the multi-class classification problem using boosting algorithms.\n",
    "\n",
    "**Dataset description**: White Wine Quality dataset contains informations related to white and white variants of the Portuguese \"Vinho Verde\" wine.\n",
    "\n",
    "Input variables (based on physicochemical tests):\n",
    "1. fixed acidity\n",
    "1. volatile acidity\n",
    "1. citric acid\n",
    "1. residual sugar\n",
    "1. chlorides\n",
    "1. free sulfur dioxide\n",
    "1. total sulfur dioxide\n",
    "1. density\n",
    "1. pH\n",
    "1. sulphates\n",
    "1. alcohol\n",
    "\n",
    "Output variable (based on sensory data):\n",
    "12. quality (score between 0 and 10)\n",
    "\n",
    "**Step 1: Data Loading**\n",
    "\n",
    "1. Load the White Wine Quality (please omit Red Wine Quality) dataset.\n",
    "\n",
    "**Step 2: Data Preprocessing and Splitting**\n",
    "\n",
    "2. Create a target variable for multi-class classification. You can categorize wine quality scores into classes (e.g., low, medium, high) or keep them as is (but please use at least 3 classes).\n",
    "\n",
    "3. Split dataset into features (X) and the target variable (y).\n",
    "\n",
    "4. Split the data into training and testing sets using an 80-20 or similar ratio.\n",
    "\n",
    "5. Perform any necessary SEDA, data preprocessing and feature engineering.\n",
    "\n",
    "**Step 3: Feature Selection**\n",
    "\n",
    "6. Apply initial feature selection techniques.\n",
    "\n",
    "**Step 4: Model Training and Hyperparameter Tuning**\n",
    "\n",
    "7. Train and fine-tune the following models separately using only your training data (is the data well balanced? if not maybe we can take this into account using some hyperparameters or we should rebalance our dataset using techniques from ML1 course?):\n",
    "\n",
    "   * XGBoost\n",
    "   * LightGBM\n",
    "   * CatBoost\n",
    "   * Classical Gradient Boosting Model\n",
    "   * extra task: please use sklearn Histogram-Based Gradient Boosting model (alternative for LightGBM)\n",
    "Please consider which evaluation metrics (https://scikit-learn.org/stable/modules/model_evaluation.html#from-binary-to-multiclass-and-multilabel) and cost functions to choose!!! Your are solving multiclass classification problem!\n",
    "\n",
    "8. Try to conduct the feature selection process based on the measures built into the models. Create a variable that is pure noise and does not add any value to the model (draw it from some distribution, e.g. a uniform distribution) and add it to the training set. If any variable is less important than the created noise, please remove it from the model. Of course, at the end, remove this noise from the model as well.\n",
    "\n",
    "\n",
    "9. For each model, perform hyperparameter tuning using techniques like grid search or random search with cross-validation (e.g., GridSearchCV or RandomizedSearchCV). Tune hyperparameters such as learning rate, maximum depth, number of estimators, and any **model-specific hyperparameters - first of all, focus on HP that are related to overfitting**.\n",
    "\n",
    "You can do task 8 and 9 in parallel, because testing variables should also be related to testing hyperparameters! Complete tasks 8 and 9 iteratively. Try different configurations.\n",
    "\n",
    "**Step 5: Model Comparison**\n",
    "\n",
    "10. Evaluate the tuned models on the  testing datasets using appropriate classification metrics (e.g., accuracy, precision, recall, F1-score) - please remember that you are solving multi-class classification problem so you have choose proper evaluation strategy - sometimes confusion matrix is the best!!!.\n",
    "\n",
    "11. Compare the performance of XGBoost, LightGBM, CatBoost, and Classical Gradient Boosting. Discuss which model performed the best for our dataset and why."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f9312c",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3e89a755",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ucimlrepo import fetch_ucirepo\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60762707",
   "metadata": {},
   "source": [
    "# Step 1: Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e3941e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ucimlrepo in c:\\users\\afat\\anaconda3\\lib\\site-packages (0.0.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install ucimlrepo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ca947201",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch dataset\n",
    "wine_quality = fetch_ucirepo(id=186)\n",
    "X = wine_quality.data.features\n",
    "y = wine_quality.data.targets['quality']  # 'quality' is the target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f71d9197",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    6\n",
       "1    6\n",
       "2    6\n",
       "3    6\n",
       "4    6\n",
       "Name: quality, dtype: int64"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2e2872",
   "metadata": {},
   "source": [
    "# Step 2: Data Preprocessing and Splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6fcb09",
   "metadata": {},
   "source": [
    "**2. Create a target variable for multi-class classification. You can categorize wine quality scores into classes (e.g., low, medium, high) or keep them as is (but please use at least 3 classes).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "927d3a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to categorize wine quality scores into classes\n",
    "def categorize_quality(quality):\n",
    "    if quality <= 4:\n",
    "        return \"low\"\n",
    "    elif quality <= 6:\n",
    "        return \"medium\"\n",
    "    else:\n",
    "        return \"high\"\n",
    "\n",
    "# Apply the categorize_quality function to create the 'class' target variable\n",
    "y_class = y.apply(categorize_quality)\n",
    "\n",
    "# Add the 'class' target variable to the DataFrame\n",
    "data['class'] = y_class\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5665b5e0",
   "metadata": {},
   "source": [
    "**4. Split the data into training and testing sets using an 80-20 or similar ratio.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "03794c56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (3918, 11)\n",
      "X_test shape: (980, 11)\n",
      "y_train shape: (3918,)\n",
      "y_test shape: (980,)\n"
     ]
    }
   ],
   "source": [
    "# Split the data into training and testing sets (80% training, 20% testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Check the shapes of the resulting sets\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"y_test shape:\", y_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3fbd61",
   "metadata": {},
   "source": [
    "**5. Perform any necessary SEDA, data preprocessing and feature engineering.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
